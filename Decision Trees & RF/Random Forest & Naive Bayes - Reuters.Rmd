---
title: "Exercises 2"
author: "Erik Kahnke"
date: "August 18, 2015"
output: word_document
---

##Author Attribution

I decided to build a Naive Bayes model and a Random Forest model to predict the author of an article on the basis of that article's textual content. To do this, I had to read in and preprocess the corpus training data. I then created a document term matrix and removed the sparse terms and did the same when reading in the test data. Next, I converted the document term matrices into data frames for the classsifier models.

```{r, include=FALSE}
library(tm)
library(randomForest)
library(e1071)
library(rpart)
library(caret)
library(plyr)
```

```{r, echo=FALSE}
readerPlain = function(fname){
  readPlain(elem=list(content=readLines(fname)), id=fname, language='en') }
author_dirs = Sys.glob('ReutersC50/C50train/*')
file_list = NULL
train_labels = NULL
for(author in author_dirs) {
  author_name = substring(author, first=23)
  files_to_add = Sys.glob(paste0(author, '/*.txt'))
  file_list = append(file_list, files_to_add)
  train_labels = append(train_labels, rep(author_name, length(files_to_add)))
}
all_docs = lapply(file_list, readerPlain) 
names(all_docs) = file_list
names(all_docs) = sub('.txt', '', names(all_docs))
train_corpus = Corpus(VectorSource(all_docs))
names(train_corpus) = file_list
train_corpus = tm_map(train_corpus, content_transformer(tolower)) 
train_corpus = tm_map(train_corpus, content_transformer(removeNumbers)) 
train_corpus = tm_map(train_corpus, content_transformer(removePunctuation)) 
train_corpus = tm_map(train_corpus, content_transformer(stripWhitespace)) 
train_corpus = tm_map(train_corpus, content_transformer(removeWords), stopwords("SMART"))
DTM_train = DocumentTermMatrix(train_corpus)
DTM_train = removeSparseTerms(DTM_train, 0.975)
author_dirs = Sys.glob('ReutersC50/C50test/*')
file_list = NULL
test_labels = NULL
for(author in author_dirs) {
  author_name = substring(author, first=22)
  files_to_add = Sys.glob(paste0(author, '/*.txt'))
  file_list = append(file_list, files_to_add)
  test_labels = append(test_labels, rep(author_name, length(files_to_add)))
}
all_docs = lapply(file_list, readerPlain) 
names(all_docs) = file_list
names(all_docs) = sub('.txt', '', names(all_docs))
test_corpus = Corpus(VectorSource(all_docs))
names(test_corpus) = file_list
test_corpus = tm_map(test_corpus, content_transformer(tolower)) 
test_corpus = tm_map(test_corpus, content_transformer(removeNumbers)) 
test_corpus = tm_map(test_corpus, content_transformer(removePunctuation)) 
test_corpus = tm_map(test_corpus, content_transformer(stripWhitespace)) 
test_corpus = tm_map(test_corpus, content_transformer(removeWords), stopwords("SMART"))
reuters_dict = NULL
reuters_dict = dimnames(DTM_train)[[2]]
DTM_test = DocumentTermMatrix(test_corpus, list(dictionary=reuters_dict))
DTM_test = removeSparseTerms(DTM_test, 0.975)
DTM_train_df = as.data.frame(inspect(DTM_train))
DTM_test_df = as.data.frame(inspect(DTM_test))
```

Now let's apply the Naive Bayes model and analyze the results. Below is a visual representation of the frequency of predicted authors versus the actual authors. We can see that Naive Bayes does not appear to be able to predict authors with high accuracy.
```{r, echo=FALSE}
model_NB = naiveBayes(x=DTM_train_df, y=as.factor(train_labels), laplace=1)
predict_NB = predict(model_NB, DTM_test_df)
table_NB = as.data.frame(table(predict_NB,test_labels))
plot = ggplot(table_NB)
plot + geom_tile(aes(x=test_labels, y=predict_NB, fill=Freq)) + 
  scale_x_discrete(name="Actual Author") + 
  scale_y_discrete(name="Predicted Author") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
confusion_NB = confusionMatrix(table(predict_NB,test_labels))
```

This is confirmed below. The Naive Bayes model has an accuracy of 18.52%.

```{r, echo=FALSE}
confusion_NB$overall
```

Now let's apply the Random Forest model and analyze the results. Below is a visual representation of the frequency of predicted authors versus the actual authors. We can see that Random Forest does a lot better at predicting authors with a higher accuracy.

```{r, echo=FALSE}
DTM_test = as.matrix(DTM_test)
DTM_train = as.matrix(DTM_train)
x <- data.frame(DTM_test[,intersect(colnames(DTM_test), colnames(DTM_train))])
y <- read.table(textConnection(""), col.names = colnames(DTM_train), colClasses = "integer")
DTM_test_clean = rbind.fill(x, y)
DTM_test_df = as.data.frame(DTM_test_clean)
model_RF = randomForest(x=DTM_train_df, y=as.factor(train_labels), mtry=3, ntree=200)
predict_RF = predict(model_RF, data=DTM_test_clean)
table_RF = as.data.frame(table(predict_RF,test_labels))
plot = ggplot(table_RF)
plot + geom_tile(aes(x=test_labels, y=predict_RF, fill=Freq)) + 
  scale_x_discrete(name="Actual Author") + 
  scale_y_discrete(name="Predicted Author") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
confusion_RF = confusionMatrix(table(predict_RF,test_labels))
```

This is also confirmed below. The Random Forest model has an accuracy of 70.92%.

```{r, echo=FALSE}
confusion_RF$overall
```

I prefer to use the random forest model because it predicts the author at a higher accuracy of 70.92% and does not overfit.

To identify any authors whose articles seem difficult to distinguish from one another, I will look at the confusion matrices for the  sensitivity of each each author class for both models. 
```{r, echo=FALSE}
confusion_RF_df = as.data.frame(confusion_RF$byClass)
confusion_RF_df[order(-confusion_RF_df$Sensitivity),1:2]
confusion_NB_df = as.data.frame(confusion_NB$byClass)
confusion_NB_df[order(-confusion_NB_df$Sensitivity),1:2]
```
The authors whose articles seem difficult to distinguish from one another are Brad Dorfman, Edna Fernandes, Jane Macartney, Jonathan Birt, Keith Weir, Kevin Drawbaugh, Kirstin Ridley, Mure Dickie, Scott Hillis and William Kazer.
```{r, echo=FALSE}
detach("package:tm", unload=TRUE)
```
